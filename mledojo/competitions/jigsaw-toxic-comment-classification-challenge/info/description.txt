### Description

Engaging in meaningful discussions online can be challenging due to the prevalence of abuse and harassment, which often leads individuals to refrain from sharing their thoughts. Many platforms struggle to foster healthy conversations, resulting in restricted or eliminated user comments. 

The Conversation AI team, a collaborative research initiative by Jigsaw and Google, aims to enhance online dialogue through innovative tools. A key focus is on understanding and mitigating negative online behaviors, particularly toxic commentsâ€”those that are rude, disrespectful, or likely to deter participation. While they have developed various publicly available models via the Perspective API, including one for toxicity detection, these models still have limitations and do not allow users to specify the types of toxicity they wish to monitor.

In this competition, participants are tasked with creating a multi-headed model that can more accurately identify various forms of toxicity, such as threats, obscenity, insults, and identity-based hate, compared to the existing Perspective models. The dataset comprises comments from Wikipedia's talk page edits, and advancements in this model could lead to more constructive and respectful online discussions.

**Disclaimer:** The dataset may contain text that is profane, vulgar, or offensive.

### Evaluation

Update: Jan 30, 2018. Due to changes in the competition dataset, we have changed the evaluation metric of this competition.

Submissions are now evaluated on the mean column-wise ROC AUC. In other words, the score is the average of the individual AUCs of each predicted column.

## Submission File

For each id in the test set, you must predict a probability for each of the six possible types of comment toxicity (toxic, severe_toxic, obscene, threat, insult, identity_hate). The columns must be in the same order as shown below. The file should contain a header and have the following format:

```
id,toxic,severe_toxic,obscene,threat,insult,identity_hate
00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.5
0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5
etc.
```

### Dataset Description

You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:

- toxic
- severe_toxic
- obscene
- threat
- insult
- identity_hate

You must create a model which predicts a probability of each type of toxicity for each comment.

## File descriptions

- train.csv - the training set, contains comments with their binary labels
- test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.
- sample_submission.csv - a sample submission file in the correct format
- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)

### Other Important Information

Participants should be aware of the sensitive nature of the dataset and the potential for offensive content.
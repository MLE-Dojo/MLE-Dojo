Description

Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!

Evaluation

Submissions are evaluated on the area under the ROC curve between the predicted probability and the observed target.

Submission File

For each id in the test set, you must predict the probability for the target variable defects. The file should contain a header and have the following format:

id, defects  
101763, 0.5  
101764, 0.5  
101765, 0.5  
etc.

Dataset Description

The dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.

Files

train.csv - the training dataset; defects is the binary target, which is treated as a boolean (False=0, True=1)  
test.csv - the test dataset; your objective is to predict the probability of positive defects (i.e., defects=True)  
sample_submission.csv - a sample submission file in the correct format  

**Description**

Essay writing is an important method to evaluate student learning and performance. It is also time-consuming for educators to grade by hand. Automated Writing Evaluation (AWE) systems can score essays to supplement an educatorâ€™s other efforts. AWEs also allow students to receive regular and timely feedback on their writing. However, due to their costs, many advancements in the field are not widely available to students and educators. Open-source solutions to assess student writing are needed to reach every community with these important educational tools.

Previous efforts to develop open-source AWEs have been limited by small datasets that were not nationally diverse or focused on common essay formats. The first Automated Essay Scoring competition scored student-written short-answer responses; however, this is a writing task not often used in the classroom. To improve upon earlier efforts, a more expansive dataset that includes high-quality, realistic classroom writing samples was required. Further, to broaden the impact, the dataset should include samples across economic and location populations to mitigate the potential of algorithmic bias.

In this competition, you will work with the largest open-access writing dataset aligned to current standards for student-appropriate assessments. Can you help produce an open-source essay scoring algorithm that improves upon the original Automated Student Assessment Prize (ASAP) competition hosted in 2012?

Competition host Vanderbilt University is a private research university in Nashville, Tennessee. For this competition, Vanderbilt has partnered with The Learning Agency Lab, an Arizona-based independent nonprofit focused on developing the science of learning-based tools and programs for the social good.

To ensure the results of this competition are widely available, winning solutions will be released as open source. More robust and accessible AWE options will help more students get the frequent feedback they need and provide educators with additional support, especially in underserved districts.

**Acknowledgments**

Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and the Chan Zuckerberg Initiative for their support in making this work possible.

**Evaluation**

Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.

The quadratic weighted kappa is calculated as follows:

First, an N x N histogram matrix O is constructed, such that O_i,j corresponds to the number of essay_ids i (actual) that received a predicted value j.

An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted values:

\[ w_{i,j} = \frac{(i - j)^2 }{(N - 1)^2} \]

An N-by-N histogram matrix of expected outcomes, E, is calculated assuming that there is no correlation between values. This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum.

From these three matrices, the quadratic weighted kappa is calculated as:

\[ \kappa = 1 - \frac{\sum_{i,j} w_{i,j} O_{i,j}}{\sum_{i,j} w_{i,j} E_{i,j}} \]

**Submission File**

For each essay_id in the test set, you must predict the corresponding score (described on the Data page). The file should contain a header and have the following format:

```
essay_id,score
000d118,3
000fe60,3
001ab80,4
...
```

**Dataset Description**

The competition dataset comprises about 24,000 student-written argumentative essays. Each essay was scored on a scale of 1 to 6 ([Link to the Holistic Scoring Rubric](#)). Your goal is to predict the score an essay received from its text.

**File and Field Information**

- **train.csv**
  - Essays and scores to be used as training data.
  - **essay_id** - The unique ID of the essay
  - **full_text** - The full essay response
  - **score** - Holistic score of the essay on a 1-6 scale

- **test.csv**
  - The essays to be used as test data. Contains the same fields as train.csv, aside from exclusion of the score. (Note: The rerun test set has approximately 8k observations.)

- **sample_submission.csv**
  - A submission file in the correct format.
  - **essay_id** - The unique ID of the essay
  - **score** - The predicted holistic score of the essay on a 1-6 scale

Please note that this is a **Code Competition**.
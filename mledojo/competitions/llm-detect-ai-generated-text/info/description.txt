### Description

Join us in a groundbreaking competition aimed at developing a model that distinguishes between essays written by middle and high school students and those generated by large language models (LLMs). As LLMs become more prevalent, concerns about their potential to facilitate plagiarism and hinder students' skill development have emerged. This competition seeks to address these issues by identifying unique artifacts in LLM-generated texts, ultimately enhancing the detection of AI-generated content. Participants will work with a diverse set of essays on various topics, simulating real-world detection scenarios and fostering the development of features that generalize across different generative models. This initiative is a collaboration between Vanderbilt University and The Learning Agency Lab.

### Evaluation

Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.

## Submission File

For each id in the test set, you must predict a probability that that essay was generated. The file should contain a header and have the following format:

```
id,generated
0000aaaa,0.1
1111bbbb,0.9
2222cccc,0.4
...
```

### Dataset Description

The competition dataset consists of approximately 10,000 essays, with a mix of student-written and LLM-generated content. Participants are tasked with determining whether each essay was produced by an LLM. The essays respond to one of seven prompts, with students instructed to read source texts before writing their responses. The training set includes essays from two prompts, primarily authored by students, while the hidden test set contains around 9,000 essays, both student-written and LLM-generated. Note that this is a Code Competition, and the test_essays.csv file contains dummy data for solution development.

## File and Field Information

- {test|train}_essays.csv
  - id - A unique identifier for each essay.
  - prompt_id - Identifies the prompt the essay was written in response to.
  - text - The essay text itself.
  - generated - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.
- train_prompts.csv - Essays were written in response to information in these fields.
  - prompt_id - A unique identifier for each prompt.
  - prompt_name - The title of the prompt.
  - instructions - The instructions given to students.
  - source_text - The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in 0 Paragraph one.\n\n1 Paragraph two.. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like # Title. When an author is indicated, their name will be given in the title after by. Not all articles have authors indicated. An article may have subheadings indicated like ## Subheading.
- sample_submission.csv - A submission file in the correct format. See the Evaluation page for details.

### Other Important Information

This is a Code Competition, and submissions must be made through Notebooks. Ensure your notebook meets the specified runtime and internet access conditions.
### Description

The competition invites participants to tackle challenging science-based questions generated by a Large Language Model (LLM), inspired by the OpenBookQA dataset. This initiative aims to enhance understanding of LLMs' self-assessment capabilities and their potential in resource-constrained environments. As LLMs evolve, traditional NLP benchmarks often fall short, prompting the need for more complex tasks to evaluate these powerful models. The dataset comprises multiple-choice questions crafted by GPT-3.5 from scientific texts sourced from Wikipedia, with a focus on filtering out simpler questions. The competition provides a unique opportunity to explore the performance of smaller models against larger ones, particularly in a Kaggle environment with GPU and time constraints.

### Evaluation

Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):

MAP@3 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,3)} P(k) \times rel(k)

where U  is the number of questions in the test set, P(k) is the precision at cutoff k, n is the number predictions per question, and rel(k) is an indicator function equaling 1 if the item at rank k is a relevant (correct) label, zero otherwise.

Once a correct label has been scored for an individual question in the test set, that label is no longer considered relevant for that question, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.

```
[A, B, C, D, E]
[A, A, A, A, A]
[A, B, A, C, A]
```

## Submission File

For each id in the test set, you may predict up to 3 labels for your prediction. The file should contain a header and have the following format:

```
id,prediction
0,A B C
1,B C A
2,C A B
etc.
```

### Dataset Description

In this competition, participants will answer multiple-choice questions generated by an LLM. While the exact process for generating these questions is not disclosed, 200 sample questions with answers are provided to illustrate the format and types of questions included. However, participants should be aware of a potential distributional shift between the sample and actual test set, making broad generalization essential for success. Each question features a prompt, five options (A, B, C, D, E), and a designated correct answer. The competition employs a hidden test, where the actual test data will be revealed upon scoring submissions, consisting of approximately 4,000 unique questions.

### Files

- train.csv - a set of 200 questions with the answer column
- test.csv - the test set; your task it to predict the top three most probable answers given the prompt. NOTE: the test data you see here just a copy of the training data without the answers. The unseen re-run test set is comprised of ~4,000 different prompts.
- sample_submission.csv - a sample submission file in the correct format

## Columns

- prompt - the text of the question being asked
- A - option A; if this option is correct, then answer will be A
- B - option B; if this option is correct, then answer will be B
- C - option C; if this option is correct, then answer will be C
- D - option D; if this option is correct, then answer will be D
- E - option E; if this option is correct, then answer will be E
- answer -  the most correct answer, as defined by the generating LLM (one of A, B, C, D, or E).

### Other Important Information

This is a Code Competition. Refer to Code Requirements for details.